{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f579cf25",
   "metadata": {},
   "source": [
    "#Ans1.) \n",
    "\n",
    "R-squared, also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable (target) that is explained by the independent variables (features) in a regression model.\n",
    "\n",
    "R-squared values range from 0 to 1, where:\n",
    "\n",
    "0 indicates that the model does not explain any of the variability of the response data around its mean.\n",
    "1 indicates that the model explains all the variability of the response data around its mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db087097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "628fe6a4",
   "metadata": {},
   "source": [
    "#Ans2.)\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model. It penalizes the addition of unnecessary variables to the model, thereby providing a more accurate measure of the goodness of fit.\n",
    "\n",
    "Adjusted R-squared accounts for the number of predictors in the model, and it tends to be lower than the regular R-squared when additional predictors are added that do not significantly improve the model's fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f182276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04fb07ea",
   "metadata": {},
   "source": [
    "#Ans3.) \n",
    "\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors or when evaluating the goodness of fit of models with different complexity levels. It provides a more conservative estimate of the explanatory power of the model by penalizing the addition of unnecessary predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d9a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9170a783",
   "metadata": {},
   "source": [
    "#Ans4.) \n",
    "\n",
    "RMSE (Root Mean Squared Error): RMSE is a measure of the average deviation of the predicted values from the actual values in a regression model. It is calculated by taking the square root of the average of the squared differences between the predicted and actual values\n",
    "\n",
    "MSE (Mean Squared Error): MSE is similar to RMSE but without taking the square root. It represents the average of the squared differences between the predicted and actual values\n",
    "\n",
    "MAE (Mean Absolute Error): MAE is a measure of the average absolute deviation of the predicted values from the actual values. It is calculated by taking the average of the absolute differences between the predicted and actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c953b66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a471b504",
   "metadata": {},
   "source": [
    "#Ans5.) \n",
    "\n",
    "Advantages:\n",
    "\n",
    "RMSE, MSE, and MAE are all intuitive measures of model performance that quantify the error between predicted and actual values.\n",
    "They are widely used and easily interpretable.\n",
    "RMSE penalizes large errors more than smaller errors due to the squaring operation, which may be desirable in certain applications.\n",
    "MAE is more robust to outliers compared to RMSE.\n",
    "Disadvantages:\n",
    "\n",
    "RMSE and MSE are sensitive to outliers and large errors, which may not always reflect the true performance of the model.\n",
    "RMSE and MSE are influenced by the scale of the dependent variable, making it difficult to compare models across different datasets.\n",
    "MAE does not provide as much information about the magnitude of errors as RMSE, which may be a limitation in certain applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff08bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "747ff611",
   "metadata": {},
   "source": [
    "#Ans6.) \n",
    "\n",
    "Lasso regularization (Least Absolute Shrinkage and Selection Operator) is a technique used in linear regression to penalize the absolute size of the coefficients. It adds a penalty term to the loss function, forcing some of the coefficients to shrink to zero, effectively performing variable selection.\n",
    "\n",
    "Differences from Ridge regularization:\n",
    "\n",
    "Lasso regularization penalizes the absolute value of the coefficients (L1 penalty), while Ridge regularization penalizes the square of the coefficients (L2 penalty).\n",
    "Lasso tends to shrink some coefficients to exactly zero, effectively performing feature selection, while Ridge only shrinks coefficients towards zero.\n",
    "\n",
    "When to use Lasso regularization:\n",
    "\n",
    "Lasso regularization is more appropriate when there is a need for feature selection or when dealing with datasets with a large number of features. It can help simplify the model by removing irrelevant features and improving interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1742ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d47b09a4",
   "metadata": {},
   "source": [
    "#Ans7.)\n",
    "\n",
    "Regularized linear models (such as Ridge and Lasso regression) help prevent overfitting by adding a penalty term to the loss function, which discourages the model from learning complex relationships that may not generalize well to unseen data.\n",
    "\n",
    "Example: In Ridge regression, the penalty term is proportional to the square of the coefficients. By penalizing large coefficients, Ridge regression effectively reduces the model's flexibility and prevents it from fitting the noise in the training data too closely, thereby reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a2dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90994970",
   "metadata": {},
   "source": [
    "#Ans8.) \n",
    "\n",
    "Limitations:\n",
    "\n",
    "Regularized linear models assume that the relationship between the features and the target variable is linear. If the true relationship is nonlinear, regularized linear models may not capture it effectively.\n",
    "Regularized linear models require careful tuning of the regularization parameter, which can be challenging and computationally expensive, especially for large datasets.\n",
    "Regularized linear models may not perform well if there is multicollinearity among the features, as they tend to shrink correlated coefficients towards zero indiscriminately.\n",
    "Not always the best choice:\n",
    "\n",
    "Regularized linear models may not be suitable for all types of datasets or modeling tasks. For example, if the relationship between the features and the target variable is highly nonlinear, other nonlinear regression techniques may be more appropriate.\n",
    "Regularized linear models may not provide the best interpretability, especially when feature selection is performed automatically. In some cases, simpler linear models without regularization may be preferred for their transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b7d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab47d1ca",
   "metadata": {},
   "source": [
    "#Ans9.) \n",
    "\n",
    "Choice of better performer:\n",
    "\n",
    "It depends on the specific context and priorities of the problem. If minimizing large errors is critical, Model A with lower RMSE may be preferred. If the focus is on minimizing the average magnitude of errors, Model B with lower MAE may be preferred.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "RMSE and MAE measure different aspects of the error distribution and may lead to different conclusions about model performance. RMSE penalizes large errors more than MAE, which may or may not align with the goals of the analysis.\n",
    "The choice of metric should consider the specific requirements and objectives of the problem, as well as any domain-specific considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d7857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b533dea1",
   "metadata": {},
   "source": [
    "#Ans10.)\n",
    "\n",
    "Choice of better performer:\n",
    "\n",
    "It depends on the specific characteristics of the dataset and the goals of the analysis. If the dataset contains many irrelevant features that can be removed, Lasso regularization (Model B) may be preferred due to its feature selection capability. If multicollinearity among the features is a concern, Ridge regularization (Model A) may be preferred for its ability to handle correlated predictors more gracefully.\n",
    "Trade-offs and limitations:\n",
    "\n",
    "Ridge regularization tends to shrink all coefficients towards zero gradually, but it rarely sets coefficients exactly to zero, making it less effective for feature selection compared to Lasso regularization.\n",
    "Lasso regularization can lead to sparse models with fewer features by setting some coefficients exactly to zero, but it may not perform well when there is multicollinearity among the features or when all features are relevant for prediction. Additionally, the choice of the regularization parameter (e.g., the strength of regularization) can affect the performance of the model and may require careful tuning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304b111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e62485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
